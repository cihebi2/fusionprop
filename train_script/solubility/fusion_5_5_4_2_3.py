import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import random
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import KFold
from torch.cuda.amp import autocast, GradScaler
from pathlib import Path
import warnings
from tqdm.auto import tqdm
from torch.optim.lr_scheduler import OneCycleLR
from torch.optim import AdamW
import matplotlib.pyplot as plt
import datetime

torch.backends.cudnn.benchmark = True
torch.cuda.empty_cache()
warnings.filterwarnings("ignore")

# æ”¹è¿›çš„é…ç½®ç±» - æ·»åŠ ç‰¹å¾å½’ä¸€åŒ–é€‰é¡¹
class NormalizedConfig:
    def __init__(self, **kwargs):
        # ç‰¹å¾è·¯å¾„
        self.esm2_train_dir = "./esm2_features_train"  
        self.esm2_test_dir = "./esm2_features_test"    
        self.esmc_train_dir = "./esmc_features_train"   
        self.esmc_test_dir = "./esmc_features_test"     
        self.model_save_dir = "./fusion_models_5_5_4_2_3"   
        
        # æ·»åŠ é…¿é…’é…µæ¯æµ‹è¯•é›†è·¯å¾„
        self.esm2_cerevisiae_dir = "./esm2_features_cerevisiae/"  # S-PLMé…¿é…’é…µæ¯æµ‹è¯•ç‰¹å¾ç›®å½•
        self.esmc_cerevisiae_dir = "./esmc_features_cerevisiae/"     # ESM-Cé…¿é…’é…µæ¯æµ‹è¯•ç‰¹å¾ç›®å½• 
        
        # è®­ç»ƒå‚æ•° - é»˜è®¤å€¼ï¼Œå¯ä»¥é€šè¿‡kwargsä¿®æ”¹
        self.batch_size = 32
        self.epochs = 15
        self.lr = 5e-5
        self.weight_decay = 1e-6
        self.max_seq_len = 1400
        
        # æ¨¡å‹å‚æ•° - é»˜è®¤å€¼ï¼Œå¯ä»¥é€šè¿‡kwargsä¿®æ”¹
        self.esm2_dim = 1280
        self.esmc_dim = 1152
        self.hidden_dim = 512
        self.dropout = 0.1
        self.head_dropout = 0.2
        
        # ç‰¹å¾å½’ä¸€åŒ–å‚æ•° - é»˜è®¤å€¼ï¼Œå¯ä»¥é€šè¿‡kwargsä¿®æ”¹
        self.normalize_features = True
        self.normalization_method = "global"
        # é¢„è®¡ç®—çš„ç»Ÿè®¡å€¼ (å°†åœ¨é¦–æ¬¡è¿è¡Œæ•°æ®é›†æ—¶å¡«å……)
        self.esm2_mean = 0.0
        self.esm2_std = 1.0
        self.esmc_mean = 0.0
        self.esmc_std = 1.0
        
        # è®­ç»ƒè®¾ç½®
        self.use_amp = True
        self.grad_clip = 0.5
        self.num_workers = 6
        self.num_folds = 5
        self.random_seed = 42
        self.warmup_ratio = 0.1
        self.patience = 3    # æ—©åœè€å¿ƒå€¼å‡å°‘ä»¥åŠ é€Ÿä¼˜åŒ–
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è°ƒè¯•é€‰é¡¹
        self.visualize_features = False

        # ä½¿ç”¨kwargsæ›´æ–°é…ç½®
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                print(f"è­¦å‘Š: é…ç½®ä¸­ä¸å­˜åœ¨å±æ€§ '{key}'")

    def set_seed(self):
        random.seed(self.random_seed)
        np.random.seed(self.random_seed)
        torch.manual_seed(self.random_seed)
        torch.cuda.manual_seed_all(self.random_seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# å…¶ä»–ç±»å’Œå‡½æ•°ä¿æŒä¸å˜
# NormalizedDatasetç±»ã€normalized_collate_fnå‡½æ•°ã€WeightedFusionModelç±»éƒ½ä¿æŒåŸæ ·
class NormalizedDataset(Dataset):
    def __init__(self, esm2_dir, esmc_dir, config, debug=True, compute_stats=False):
        self.esm2_dir = esm2_dir
        self.esmc_dir = esmc_dir
        self.config = config
        self.debug = debug
        
        # æ‰¾å‡ºå…±æœ‰æ ·æœ¬ID
        self.esm2_files = {f.split('_features')[0]: f for f in os.listdir(esm2_dir) if f.endswith("_features.npy")}
        self.esmc_files = {f.split('_features')[0]: f for f in os.listdir(esmc_dir) if f.endswith("_features.npy")}
        self.common_ids = sorted(list(set(self.esm2_files.keys()) & set(self.esmc_files.keys())))
        print(f"æ‰¾åˆ° {len(self.common_ids)} ä¸ªå…±æœ‰æ ·æœ¬")
        
        # è®¡ç®—ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        if compute_stats:
            self._compute_feature_stats()
        
        # é¦–ä¸ªæ ·æœ¬åˆ†æ
        if debug:
            self._analyze_sample(0)

    def _compute_feature_stats(self):
        """è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯"""
        print("è®¡ç®—ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯...")
        
        # æ”¶é›†æ ·æœ¬
        esm2_samples = []
        esmc_samples = []
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥åŠ å¿«è®¡ç®—
        sample_count = min(100, len(self.common_ids))
        
        for idx in tqdm(range(sample_count), desc="æ”¶é›†ç‰¹å¾æ ·æœ¬"):
            try:
                # åŠ è½½å•ä¸ªæ ·æœ¬
                sample_id = self.common_ids[idx]
                esm2_data = np.load(os.path.join(self.esm2_dir, self.esm2_files[sample_id]), allow_pickle=True).item()
                esmc_data = np.load(os.path.join(self.esmc_dir, self.esmc_files[sample_id]), allow_pickle=True).item()
                
                # æå–ç‰¹å¾
                esm2_features = esm2_data["residue_representation"]
                esmc_features = esmc_data["residue_representation"]
                mask = esm2_data["mask"]
                
                # ä»…æ”¶é›†æœ‰æ•ˆä½ç½®çš„ç‰¹å¾
                valid_esm2 = esm2_features[mask]
                valid_esmc = esmc_features[mask]
                
                esm2_samples.append(valid_esm2)
                esmc_samples.append(valid_esmc)
                
            except Exception as e:
                if self.debug:
                    print(f"å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™: {str(e)}")
        
        # åˆå¹¶æ ·æœ¬å¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        all_esm2 = np.vstack(esm2_samples) if esm2_samples else np.array([])
        all_esmc = np.vstack(esmc_samples) if esmc_samples else np.array([])
        
        if len(all_esm2) > 0:
            self.config.esm2_mean = float(np.mean(all_esm2))
            self.config.esm2_std = float(np.std(all_esm2) + 1e-6)
            self.config.esmc_mean = float(np.mean(all_esmc))
            self.config.esmc_std = float(np.std(all_esmc) + 1e-6)
            
            print(f"esm2ç‰¹å¾ç»Ÿè®¡: å‡å€¼={self.config.esm2_mean:.4f}, æ ‡å‡†å·®={self.config.esm2_std:.4f}")
            print(f"ESMCç‰¹å¾ç»Ÿè®¡: å‡å€¼={self.config.esmc_mean:.4f}, æ ‡å‡†å·®={self.config.esmc_std:.4f}")
        else:
            print("æ— æ³•è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")

    def _analyze_sample(self, idx):
        """è¯¦ç»†åˆ†ææŒ‡å®šæ ·æœ¬ï¼Œç‰¹åˆ«å…³æ³¨ç‰¹å¾åˆ†å¸ƒ"""
        if idx >= len(self.common_ids):
            print("æ ·æœ¬ç´¢å¼•è¶…å‡ºèŒƒå›´")
            return
            
        sample_id = self.common_ids[idx]
        print(f"\n===== æ ·æœ¬åˆ†æ: {sample_id} =====")
        
        try:
            # åŠ è½½åŸå§‹ç‰¹å¾
            esm2_data = np.load(os.path.join(self.esm2_dir, self.esm2_files[sample_id]), allow_pickle=True).item()
            esmc_data = np.load(os.path.join(self.esmc_dir, self.esmc_files[sample_id]), allow_pickle=True).item()
            
            # è¾“å‡ºé”®å
            print("esm2æ•°æ®é”®:", list(esm2_data.keys()))
            print("ESMCæ•°æ®é”®:", list(esmc_data.keys()))
            
            # ç‰¹å¾å½¢çŠ¶
            esm2_features = esm2_data["residue_representation"]
            esmc_features = esmc_data["residue_representation"]
            print(f"esm2ç‰¹å¾å½¢çŠ¶: {esm2_features.shape}, ç±»å‹: {esm2_features.dtype}")
            print(f"ESMCç‰¹å¾å½¢çŠ¶: {esmc_features.shape}, ç±»å‹: {esmc_features.dtype}")
            
            # åŸå§‹ç‰¹å¾ç»Ÿè®¡
            print(f"esm2ç‰¹å¾: æœ€å°å€¼={np.min(esm2_features):.4f}, æœ€å¤§å€¼={np.max(esm2_features):.4f}")
            print(f"esm2ç‰¹å¾: å‡å€¼={np.mean(esm2_features):.4f}, æ ‡å‡†å·®={np.std(esm2_features):.4f}")
            print(f"ESMCç‰¹å¾: æœ€å°å€¼={np.min(esmc_features):.4f}, æœ€å¤§å€¼={np.max(esmc_features):.4f}")
            print(f"ESMCç‰¹å¾: å‡å€¼={np.mean(esmc_features):.4f}, æ ‡å‡†å·®={np.std(esmc_features):.4f}")
            
            # æ©ç ä¿¡æ¯
            mask = esm2_data["mask"]
            print(f"æ©ç å½¢çŠ¶: {mask.shape}, ç±»å‹: {mask.dtype}")
            print(f"æœ‰æ•ˆä½ç½®æ¯”ä¾‹: {np.mean(mask):.4f}")
            
            # æ ‡ç­¾ä¿¡æ¯
            sol = esm2_data["solubility"]
            print(f"æº¶è§£åº¦æ ‡ç­¾: {sol:.4f}")
            
            # è·å–å½’ä¸€åŒ–ç‰ˆæœ¬çš„ç‰¹å¾
            normalized_esm2 = self._normalize_features(torch.from_numpy(esm2_features).float(), "esm2")
            normalized_esmc = self._normalize_features(torch.from_numpy(esmc_features).float(), "esmc")
            
            # æ‰“å°å½’ä¸€åŒ–åçš„ç»Ÿè®¡ä¿¡æ¯
            print(f"\nå½’ä¸€åŒ–åesm2ç‰¹å¾: å‡å€¼={normalized_esm2.mean().item():.4f}, æ ‡å‡†å·®={normalized_esm2.std().item():.4f}")
            print(f"å½’ä¸€åŒ–åESMCç‰¹å¾: å‡å€¼={normalized_esmc.mean().item():.4f}, æ ‡å‡†å·®={normalized_esmc.std().item():.4f}")
            
            # å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ
            if self.config.visualize_features:
                self._visualize_feature_distribution(
                    esm2_features, esmc_features, 
                    normalized_esm2.numpy(), normalized_esmc.numpy(),
                    sample_id
                )
            
        except Exception as e:
            print(f"åˆ†æå‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
        
        print("=============================")

    def _visualize_feature_distribution(self, raw_esm2, raw_esmc, norm_esm2, norm_esmc, sample_id):
        """å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ"""
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
            
            # åŸå§‹ç‰¹å¾åˆ†å¸ƒ
            axes[0,0].hist(raw_esm2.flatten(), bins=50, alpha=0.7)
            axes[0,0].set_title('åŸå§‹esm2ç‰¹å¾åˆ†å¸ƒ')
            axes[0,0].grid(True)
            
            axes[0,1].hist(raw_esmc.flatten(), bins=50, alpha=0.7)
            axes[0,1].set_title('åŸå§‹ESMCç‰¹å¾åˆ†å¸ƒ')
            axes[0,1].grid(True)
            
            # å½’ä¸€åŒ–åç‰¹å¾åˆ†å¸ƒ
            axes[1,0].hist(norm_esm2.flatten(), bins=50, alpha=0.7)
            axes[1,0].set_title('å½’ä¸€åŒ–åesm2ç‰¹å¾åˆ†å¸ƒ')
            axes[1,0].grid(True)
            
            axes[1,1].hist(norm_esmc.flatten(), bins=50, alpha=0.7)
            axes[1,1].set_title('å½’ä¸€åŒ–åESMCç‰¹å¾åˆ†å¸ƒ')
            axes[1,1].grid(True)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            plt.suptitle(f"æ ·æœ¬ {sample_id} ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”\n"
                        f"åŸå§‹esm2: Î¼={np.mean(raw_esm2):.3f}, Ïƒ={np.std(raw_esm2):.3f} | "
                        f"åŸå§‹ESMC: Î¼={np.mean(raw_esmc):.3f}, Ïƒ={np.std(raw_esmc):.3f}\n"
                        f"å½’ä¸€åŒ–esm2: Î¼={np.mean(norm_esm2):.3f}, Ïƒ={np.std(norm_esm2):.3f} | "
                        f"å½’ä¸€åŒ–ESMC: Î¼={np.mean(norm_esmc):.3f}, Ïƒ={np.std(norm_esmc):.3f}",
                        fontsize=12)
            
            plt.tight_layout()
            
            # åˆ›å»ºå›¾è¡¨ä¿å­˜ç›®å½•
            vis_dir = Path("feature_visualizations")
            vis_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜å›¾è¡¨
            plt.savefig(vis_dir / f"sample_{sample_id}_feature_distribution.png", dpi=100)
            plt.close()
            
            print(f"ç‰¹å¾åˆ†å¸ƒå›¾è¡¨å·²ä¿å­˜åˆ° feature_visualizations/sample_{sample_id}_feature_distribution.png")
            
        except Exception as e:
            print(f"å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒå¤±è´¥: {str(e)}")

    def _normalize_features(self, features, feature_type="esm2"):
        """æ ¹æ®é…ç½®çš„æ–¹æ³•å½’ä¸€åŒ–ç‰¹å¾"""
        # å¦‚æœç¦ç”¨å½’ä¸€åŒ–ï¼Œç›´æ¥è¿”å›åŸå§‹ç‰¹å¾
        if not self.config.normalize_features:
            return features
            
        # åŸºäºæ–¹æ³•é€‰æ‹©å½’ä¸€åŒ–ç­–ç•¥
        if self.config.normalization_method == "global":
            # ä½¿ç”¨å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®
            if feature_type == "esm2":
                return (features - self.config.esm2_mean) / self.config.esm2_std
            else:  # esmc
                return (features - self.config.esmc_mean) / self.config.esmc_std
                
        elif self.config.normalization_method == "sequence":
            # å¯¹æ¯ä¸ªåºåˆ—å•ç‹¬å½’ä¸€åŒ–
            mean = features.mean(dim=0, keepdim=True)
            std = features.std(dim=0, keepdim=True) + 1e-6
            return (features - mean) / std
            
        # é»˜è®¤æƒ…å†µï¼šä¸å½’ä¸€åŒ–
        return features

    def __len__(self):
        return len(self.common_ids)
    
    def __getitem__(self, idx):
        sample_id = self.common_ids[idx]
        
        try:
            # åŠ è½½ç‰¹å¾
            esm2_data = np.load(os.path.join(self.esm2_dir, self.esm2_files[sample_id]), allow_pickle=True).item()
            esmc_data = np.load(os.path.join(self.esmc_dir, self.esmc_files[sample_id]), allow_pickle=True).item()
            
            # è·å–ç‰¹å¾å’Œæ©ç 
            esm2_features = torch.from_numpy(esm2_data["residue_representation"]).float()
            esmc_features = torch.from_numpy(esmc_data["residue_representation"]).float()
            
            # ç»´åº¦è§„èŒƒåŒ–
            if esm2_features.dim() > 2:
                esm2_features = esm2_features.squeeze(0)
            if esmc_features.dim() > 2:
                esmc_features = esmc_features.squeeze(0)
                
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            assert esm2_features.dim() == 2, f"esm2ç‰¹å¾ç»´åº¦é”™è¯¯: {esm2_features.shape}"
            assert esmc_features.dim() == 2, f"ESMCç‰¹å¾ç»´åº¦é”™è¯¯: {esmc_features.shape}"
            
            # ç‰¹å¾å½’ä¸€åŒ–
            esm2_features = self._normalize_features(esm2_features, "esm2")
            esmc_features = self._normalize_features(esmc_features, "esmc")
                
            # ä½¿ç”¨S-PLMçš„æ©ç 
            mask = torch.from_numpy(esm2_data["mask"]).bool()
            if mask.dim() > 1:
                mask = mask.squeeze(0)
                
            solubility = torch.tensor(esm2_data["solubility"]).float().clamp(0.0, 1.0)
            
            return esm2_features, esmc_features, mask, solubility
            
        except Exception as e:
            if self.debug:
                print(f"åŠ è½½æ ·æœ¬ {sample_id} å‡ºé”™: {str(e)}")
            # è¿”å›ä¸€ä¸ªå°å°ºå¯¸çš„dummyæ ·æœ¬
            return torch.zeros(10, self.config.esm2_dim), torch.zeros(10, self.config.esmc_dim), torch.zeros(10, dtype=torch.bool), torch.tensor(0.5)

def normalized_collate_fn(batch):
    """åŠ å¼ºå¥å£®æ€§çš„æ‰¹å¤„ç†å‡½æ•°"""
    esm2_features, esmc_features, masks, solubilities = zip(*batch)
    
    # æŸ¥æ‰¾å½“å‰æ‰¹æ¬¡ä¸­çš„æœ€å¤§åºåˆ—é•¿åº¦
    max_len = max(feat.size(0) for feat in esm2_features)
    
    # å¡«å……æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªåºåˆ—åˆ°ç›¸åŒé•¿åº¦
    padded_esm2 = []
    padded_esmc = []
    padded_masks = []
    
    for i in range(len(esm2_features)):
        curr_len = esm2_features[i].size(0)
        
        # åˆ›å»ºå¡«å……å¼ é‡
        esm2_pad = torch.zeros(max_len, esm2_features[i].size(1))
        esmc_pad = torch.zeros(max_len, esmc_features[i].size(1))
        mask_pad = torch.zeros(max_len, dtype=torch.bool)
        
        # å¤åˆ¶æ•°æ®åˆ°å¡«å……å¼ é‡
        esm2_pad[:curr_len] = esm2_features[i]
        esmc_pad[:curr_len] = esmc_features[i]
        mask_pad[:curr_len] = masks[i]
        
        padded_esm2.append(esm2_pad)
        padded_esmc.append(esmc_pad)
        padded_masks.append(mask_pad)
    
    return {
        "esm2_features": torch.stack(padded_esm2),
        "esmc_features": torch.stack(padded_esmc),
        "mask": torch.stack(padded_masks),
        "solubility": torch.stack(solubilities)
    }

class WeightedFusionModel(nn.Module):
    def __init__(self, esm2_dim=1280, esmc_dim=1152, hidden_dim=512, dropout=0.1, use_layer_norm=True):
        super().__init__()
        self.use_layer_norm = use_layer_norm
        
        # å¯é€‰çš„å±‚å½’ä¸€åŒ–
        if use_layer_norm:
            self.esm2_norm = nn.LayerNorm(esm2_dim)
            self.esmc_norm = nn.LayerNorm(esmc_dim)
        
        # ç‰¹å¾æŠ•å½±å±‚
        self.esm2_proj = nn.Linear(esm2_dim, hidden_dim)
        self.esmc_proj = nn.Linear(esmc_dim, hidden_dim)
        
        # å¯å­¦ä¹ ç‰¹å¾æƒé‡
        self.alpha = nn.Parameter(torch.tensor([0.5]))
        
        # é¢„æµ‹å¤´
        self.head = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim//2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, esm2_features, esmc_features, mask):
        # å¯é€‰çš„å±‚å½’ä¸€åŒ–
        if self.use_layer_norm:
            esm2_features = self.esm2_norm(esm2_features)
            esmc_features = self.esmc_norm(esmc_features)
        
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        p_esm2 = self.esm2_proj(esm2_features)
        p_esmc = self.esmc_proj(esmc_features)
        
        # æ± åŒ–
        if mask is not None:
            mask = mask.unsqueeze(-1).float()
            valid_tokens = mask.sum(dim=1).clamp(min=1)
            pooled_esm2 = (p_esm2 * mask).sum(dim=1) / valid_tokens
            pooled_esmc = (p_esmc * mask).sum(dim=1) / valid_tokens
        else:
            pooled_esm2 = p_esm2.mean(dim=1)
            pooled_esmc = p_esmc.mean(dim=1)
        
        # åŠ æƒèåˆ
        alpha = torch.sigmoid(self.alpha)  # è½¬æ¢åˆ°0-1èŒƒå›´
        weighted = alpha * pooled_esm2 + (1 - alpha) * pooled_esmc
        
        # è®°å½•å½“å‰æƒé‡å€¼ (ä»…ç”¨äºåˆ†æ)
        if not self.training and hasattr(self, '_current_alpha'):
            self._current_alpha = alpha.item()
        
        # é¢„æµ‹
        return self.head(weighted).squeeze(-1)
# ä¿®æ”¹åçš„è®­ç»ƒå™¨ç±» - åªé’ˆå¯¹åŠ æƒèåˆæ¨¡å‹ä¸”ä¸ä¿å­˜æ¨¡å‹
class OptimizedTrainer:
    def __init__(self, config):
        """
        åˆå§‹åŒ–ä¼˜åŒ–è®­ç»ƒå™¨
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.device = config.device
        self.model_type = 'weighted'  # åªä½¿ç”¨åŠ æƒèåˆ
        self.scaler = GradScaler(enabled=config.use_amp)
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = Path(config.model_save_dir) / "results"  # ä¿®æ”¹ç»“æœç›®å½•
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        self._initialize_results()

    
    def _initialize_results(self):
        """åˆå§‹åŒ–ç»“æœå­—å…¸"""
        self.results = {
            "model_type": "weighted_fusion",
            "hyperparameters": {
                "hidden_dim": self.config.hidden_dim,
                "dropout": self.config.dropout,
                "lr": self.config.lr,
                "batch_size": self.config.batch_size,
                "weight_decay": self.config.weight_decay,
                "normalization_method": self.config.normalization_method
            },
            "folds": [],
            "fold_test_r2": [],
            "fold_val_r2": [],
            "fold_cerevisiae_r2": []
        }
    
    def train_kfold(self):
        """æ‰§è¡ŒKæŠ˜äº¤å‰éªŒè¯"""
        config = self.config
        
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = NormalizedDataset(
            esm2_dir=config.esm2_train_dir,
            esmc_dir=config.esmc_train_dir,
            config=config,
            debug=False,
            compute_stats=True  # è®¡ç®—å…¨å±€ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        )
        
        # åˆ›å»ºKæŠ˜åˆ†å‰²å™¨
        kf = KFold(n_splits=config.num_folds, shuffle=True, random_state=config.random_seed)
        
        # è®°å½•æ‰€æœ‰æŠ˜çš„ç»“æœ
        all_val_r2 = []
        all_test_r2 = []
        all_cerevisiae_r2 = []
        
        # è®­ç»ƒæ¯ä¸ªæŠ˜
        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(full_dataset)))):
            fold_num = fold + 1
            print(f"\nğŸ”¢ è®­ç»ƒFold {fold_num}/{config.num_folds}")
            
            # åˆ›å»ºæ•°æ®å­é›†
            train_dataset = torch.utils.data.Subset(full_dataset, train_idx)
            val_dataset = torch.utils.data.Subset(full_dataset, val_idx)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_loader = DataLoader(
                train_dataset,
                batch_size=config.batch_size,
                shuffle=True,
                collate_fn=normalized_collate_fn,
                num_workers=config.num_workers,
                pin_memory=True
            )
            
            val_loader = DataLoader(
                val_dataset,
                batch_size=config.batch_size * 2,
                shuffle=False,
                collate_fn=normalized_collate_fn,
                num_workers=config.num_workers,
                pin_memory=True
            )
            
            # åˆ›å»ºæ¨¡å‹
            model = WeightedFusionModel(
                esm2_dim=config.esm2_dim,
                esmc_dim=config.esmc_dim,
                hidden_dim=config.hidden_dim,
                dropout=config.dropout,
                use_layer_norm=True
            ).to(self.device)
            
            # è®­ç»ƒæ¨¡å‹
            val_metrics = self._train_fold(model, train_loader, val_loader, fold_num)
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_metrics = self._evaluate_test_set(model)
            
            # åœ¨é…¿é…’é…µæ¯æµ‹è¯•é›†ä¸Šè¯„ä¼°
            cerevisiae_metrics = self._evaluate_cerevisiae_set(model)
            
            # è®°å½•ç»“æœ
            fold_entry = {
                "fold_number": fold_num,
                "validation_r2": val_metrics["r2"],
                "validation_rmse": val_metrics["rmse"],
                "test_r2": test_metrics["r2"],
                "test_rmse": test_metrics["rmse"],
                "cerevisiae_r2": cerevisiae_metrics["r2"],
                "cerevisiae_rmse": cerevisiae_metrics["rmse"],
                "feature_weights": {
                    "esm2_weight": float(torch.sigmoid(model.alpha).item()),
                    "esmc_weight": float(1 - torch.sigmoid(model.alpha).item())
                }
            }
            
            self.results["folds"].append(fold_entry)
            self.results["fold_test_r2"].append(test_metrics["r2"])
            self.results["fold_val_r2"].append(val_metrics["r2"])
            self.results["fold_cerevisiae_r2"].append(cerevisiae_metrics["r2"])
            
            all_val_r2.append(val_metrics["r2"])
            all_test_r2.append(test_metrics["r2"])
            all_cerevisiae_r2.append(cerevisiae_metrics["r2"])
            
            # è¾“å‡ºå½“å‰foldç»“æœ
            alpha = torch.sigmoid(model.alpha).item()
            print(f"âœ… Fold {fold_num} ç»“æœï¼š")
            print(f"   éªŒè¯ RÂ²: {val_metrics['r2']:.4f}")
            print(f"   æµ‹è¯• RÂ²: {test_metrics['r2']:.4f}") 
            print(f"   é…¿é…’é…µæ¯ RÂ²: {cerevisiae_metrics['r2']:.4f}")
            print(f"   ç‰¹å¾æƒé‡: esm2 = {alpha:.4f}, ESM-C = {1-alpha:.4f}")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_val_r2 = np.mean(all_val_r2)
        avg_test_r2 = np.mean(all_test_r2)
        std_val_r2 = np.std(all_val_r2)
        std_test_r2 = np.std(all_test_r2)
        avg_cerevisiae_r2 = np.mean(all_cerevisiae_r2)
        std_cerevisiae_r2 = np.std(all_cerevisiae_r2)

        # ä¿å­˜èšåˆç»“æœ
        self.results["average_validation_r2"] = float(avg_val_r2)
        self.results["std_validation_r2"] = float(std_val_r2)
        self.results["average_test_r2"] = float(avg_test_r2)
        self.results["std_test_r2"] = float(std_test_r2)
        self.results["average_cerevisiae_r2"] = float(avg_cerevisiae_r2)
        self.results["std_cerevisiae_r2"] = float(std_cerevisiae_r2)

        # æ‰“å°æ€»ä½“ç»“æœ
        print(f"\nğŸ“Š åŠ æƒèåˆæ¨¡å‹æ•´ä½“ç»“æœ:")
        print(f"   éªŒè¯é›† RÂ²: {avg_val_r2:.4f} Â± {std_val_r2:.4f}")
        print(f"   æµ‹è¯•é›† RÂ²: {avg_test_r2:.4f} Â± {std_test_r2:.4f}")
        print(f"   é…¿é…’é…µæ¯æµ‹è¯•é›† RÂ²: {avg_cerevisiae_r2:.4f} Â± {std_cerevisiae_r2:.4f}")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        self._save_results()
        
        return self.results
    
    def _train_fold(self, model, train_loader, val_loader, fold_num):
        """è®­ç»ƒå•ä¸ªæŠ˜çš„æ¨¡å‹"""
        config = self.config
        device = self.device
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = AdamW(
            model.parameters(), 
            lr=config.lr,
            weight_decay=config.weight_decay
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(train_loader) * config.epochs
        scheduler = OneCycleLR(
            optimizer,
            max_lr=config.lr,
            total_steps=total_steps,
            pct_start=config.warmup_ratio,
            anneal_strategy='cos',
            div_factor=10.0,
            final_div_factor=100.0
        )
        
        # è·Ÿè¸ªæœ€ä½³æ¨¡å‹
        best_val_r2 = -float('inf')
        best_model_state = None
        patience_counter = 0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, config.epochs + 1):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_losses = []
            
            for batch in train_loader:
                # å‡†å¤‡æ•°æ®
                esm2_features = batch["esm2_features"].to(device)
                esmc_features = batch["esmc_features"].to(device)
                mask = batch["mask"].to(device)
                targets = batch["solubility"].to(device)
                
                # æ¸…é›¶æ¢¯åº¦
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­(ä½¿ç”¨æ··åˆç²¾åº¦)
                with autocast(enabled=config.use_amp):
                    outputs = model(esm2_features, esmc_features, mask)
                    loss = F.mse_loss(outputs, targets)
                
                # åå‘ä¼ æ’­(ä½¿ç”¨æ··åˆç²¾åº¦)
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                if config.grad_clip > 0:
                    self.scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
                
                # æ›´æ–°æƒé‡
                self.scaler.step(optimizer)
                self.scaler.update()
                scheduler.step()
                
                # è®°å½•æŸå¤±
                epoch_losses.append(loss.item())
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = np.mean(epoch_losses)
            
            # éªŒè¯é˜¶æ®µ
            val_metrics = self._evaluate_model(model, val_loader)
            val_r2 = val_metrics["r2"]
            val_rmse = val_metrics["rmse"]
            
            # è¾“å‡ºå½“å‰è®­ç»ƒçŠ¶æ€
            print(f"Epoch {epoch} | Loss: {avg_train_loss:.4f} | Val RÂ²: {val_r2:.4f} | Val RMSE: {val_rmse:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            if val_r2 > best_val_r2:
                best_val_r2 = val_r2
                patience_counter = 0
                best_model_state = model.state_dict().copy()
                print(f"ğŸ’¾ æ–°çš„æœ€ä½³æ¨¡å‹! RÂ²: {val_r2:.4f}")
            else:
                patience_counter += 1
                if patience_counter >= config.patience:
                    print(f"â¹ï¸ æ—©åœ: {patience_counter}ä¸ªepochæ— æ”¹å–„")
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        # æœ€ç»ˆéªŒè¯é›†è¯„ä¼°
        final_val_metrics = self._evaluate_model(model, val_loader)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        model_save_path = self.results_dir / f"best_model_fold_{fold_num}.pth"
        torch.save(model.state_dict(), model_save_path)
        print(f"ğŸ’¾ ç¬¬ {fold_num} æŠ˜çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
        
        return final_val_metrics
    
    def _evaluate_model(self, model, loader):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        model.eval()
        device = self.device
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch in loader:
                # å‡†å¤‡æ•°æ®
                esm2_features = batch["esm2_features"].to(device)
                esmc_features = batch["esmc_features"].to(device)
                mask = batch["mask"].to(device)
                targets = batch["solubility"].cpu().numpy()
                
                # å‰å‘ä¼ æ’­
                with autocast(enabled=self.config.use_amp):
                    outputs = model(esm2_features, esmc_features, mask).cpu().numpy()
                
                # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
                all_preds.append(outputs)
                all_targets.append(targets)
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹å’Œç›®æ ‡
        all_preds = np.concatenate(all_preds)
        all_targets = np.concatenate(all_targets)
        
        # è¿‡æ»¤æ— æ•ˆå€¼
        valid_mask = ~np.isnan(all_preds) & ~np.isnan(all_targets)
        clean_preds = all_preds[valid_mask]
        clean_targets = all_targets[valid_mask]
        
        # è®¡ç®—æŒ‡æ ‡
        try:
            r2 = r2_score(clean_targets, clean_preds)
            r2 = max(min(r2, 1.0), -1.0)  # çº¦æŸRÂ²èŒƒå›´
        except:
            r2 = 0.0
        
        rmse = np.sqrt(mean_squared_error(clean_targets, clean_preds))
        
        # å¦‚æœæ˜¯åŠ æƒèåˆæ¨¡å‹ï¼ŒæŠ¥å‘Šæƒé‡
        if hasattr(model, 'alpha'):
            alpha = torch.sigmoid(model.alpha).item()
            print(f"ğŸ”„ ç‰¹å¾æƒé‡: esm2 = {alpha:.4f}, ESM-C = {1-alpha:.4f}")
        
        return {"r2": float(r2), "rmse": float(rmse)}
    
    def _evaluate_test_set(self, model):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        test_dataset = NormalizedDataset(
            esm2_dir=self.config.esm2_test_dir,
            esmc_dir=self.config.esmc_test_dir,
            config=self.config,
            debug=False
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size * 2,
            shuffle=False,
            collate_fn=normalized_collate_fn,
            num_workers=self.config.num_workers
        )
        
        return self._evaluate_model(model, test_loader)
        
    def _evaluate_cerevisiae_set(self, model):
        """åœ¨é…¿é…’é…µæ¯æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        cerevisiae_dataset = NormalizedDataset(
            esm2_dir=self.config.esm2_cerevisiae_dir,
            esmc_dir=self.config.esmc_cerevisiae_dir,
            config=self.config,
            debug=False
        )
        
        cerevisiae_loader = DataLoader(
            cerevisiae_dataset,
            batch_size=self.config.batch_size * 2,
            shuffle=False,
            collate_fn=normalized_collate_fn,
            num_workers=self.config.num_workers
        )
        
        return self._evaluate_model(model, cerevisiae_loader)
    
    def _save_results(self):
        """å°†ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶"""
        results_file = self.results_dir / "results.json"
        with open(results_file, "w") as f:
            json.dump(self.results, f, indent=4)
        print(f"ç»“æœå·²ä¿å­˜åˆ° {results_file}")

# ç§»é™¤è¶…å‚æ•°ä¼˜åŒ–å‡½æ•°
# def optimize_hyperparameters(n_trials=30):
#     pass

if __name__ == "__main__":
    # è®¾ç½®matplotlibä¸­æ–‡æ”¯æŒ
    plt.rcParams["font.sans-serif"] = ["SimHei"]
    plt.rcParams["axes.unicode_minus"] = False
    
    # æŒ‡å®šæ¨¡å‹å‚æ•°
    model_params = {
        "hidden_dim": 256,
        "dropout": 0.5,
        "lr": 5e-5,
        "batch_size": 16,
        "weight_decay": 2e-5,
        "normalization_method": "sequence"
    }
    
    # åˆ›å»ºé…ç½®å®ä¾‹
    config = NormalizedConfig(**model_params)
    config.set_seed()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = OptimizedTrainer(config)
    
    # è®­ç»ƒæ¨¡å‹
    detailed_results = trainer.train_kfold()
    
    # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
    print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹ç»“æœæ‘˜è¦:")
    print(f"éªŒè¯é›† RÂ²: {detailed_results['average_validation_r2']:.4f}")
    print(f"æµ‹è¯•é›† RÂ²: {detailed_results['average_test_r2']:.4f} Â± {detailed_results['std_test_r2']:.4f}")
    print(f"é…¿é…’é…µæ¯æµ‹è¯•é›† RÂ²: {detailed_results['average_cerevisiae_r2']:.4f} Â± {detailed_results['std_cerevisiae_r2']:.4f}")